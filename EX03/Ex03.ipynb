{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>Reinforcement Learning Summer 2024</h2>\n",
    "    <h2>Prof. Dr. Frank Kirchner</h2>\n",
    "    <h4>Exercise Sheet – III</h4>\n",
    "    <h5>Due: 22.05.24</h5>\n",
    "    <hr>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.1: Eligibility Traces (10 P.)**\n",
    "\n",
    "Using the information from a reward solely to update one state does not seem efficient when we could have updated the states in the path as well. Eligibility Traces (ET) do exactly that. This task is for exploring ET and visualizing the results from using different versions of ET. For that you will use the provided custom grid world.  \n",
    "\n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"grid_world.png\" alt=\"Your Image\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "**Hand in the final code of the whole exercise and also the plotted curves for all subtasks.**\n",
    "\n",
    "1. **(1 P.)** Make yourself familiar with the environment. E.g. The observation- and action-spaces, reward, returns, etc.\n",
    "2. **(4 P.)** Implement both SARSA($\\lambda$) and WatkinsQ($\\lambda$) with the option to choose between the methods. Use an e-greedy implementation with $\\epsilon = e^{-i/\\text{max}(i)}$ where $i$ is the number of the current episode. Choose a good $\\alpha$ and $\\lambda$.\n",
    "3. **(1 P.)** Add also the option to switch between replacing and accumulating traces. Run a test with 100 episodes for each version and plot the cumulative rewards.\n",
    "4. **(2 P.)** Implement the option to print out a path (use the parameter *path* in the render functions). Path should be a list consisting of 3-tuples with the coordinates and the increase in the q-value of the taken actions. Use small circles to mark the path with their size indicating the increase.\n",
    "5. **(2 P.)** Test the previous implementation with a TD($\\lambda$) control method of your choice. Visualize the traces every ten percent of the total episodes. Run this until the agent does not run into walls anymore. Thus, find a good number of training episodes. In the end plot the cumulative reward and mark the cumulative reward with the iteration when each of the depicted trajectories started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "from gymnasium.envs.registration import register\n",
    "import warnings\n",
    "\n",
    "# Filter warnings specifically from gymnasium's core.py module\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gymnasium.core\")\n",
    "\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\", \"traces\"], \"render_fps\": 0.5}\n",
    "\n",
    "    def __init__(self, render_mode=None, seed=42):\n",
    "        self._render = False\n",
    "        self.path = []\n",
    "        super().reset(seed=seed)\n",
    "        self.size = 7  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, self.size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, self.size - 1, shape=(2,), dtype=int),\n",
    "                \"walls\": spaces.MultiBinary([self.size, self.size])\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Use fixed target location\n",
    "        self._target_location = self.np_random.integers([self.size // 2 + 1, 0], [self.size - 1, self.size - 1], 2, dtype=int)\n",
    "       \n",
    "        # Create the wall, leave out a gab in the center\n",
    "        self._walls_location = np.zeros((self.size, self.size), dtype=np.int32)\n",
    "        self._walls_location[self.size // 2] = 1\n",
    "        self._walls_location[self.size // 2, self.size // 2] = 0\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location, \"walls\": self._walls_location}\n",
    "        \n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "    \n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers([0, 0], [self.size // 2 - 1, self.size - 1], 2, dtype=int)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "\n",
    "        # Test if agent hit a wall -> Negative reward\n",
    "        if self._walls_location[tuple(self._agent_location)] == 1:\n",
    "            self._agent_location -= direction\n",
    "            reward = -0.5\n",
    "\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        reward = 1 if terminated else reward  \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\" or self._render:\n",
    "            self._render_frame(reward=reward)\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "    \n",
    "    # support funcions to get around the strict environment specifications\n",
    "    def set_render(self, render):\n",
    "        self._render = render \n",
    "\n",
    "    def render_path(self, path):\n",
    "        self._render_frame(path)\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "        elif self.render_mode == \"traces\":\n",
    "            self._render_frame()\n",
    "\n",
    "    def _render_frame(self, path=[], reward=0):\n",
    "        if self.window is None and self.render_mode in [\"human\", \"traces\"]:\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode in [\"human\", \"traces\"]:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Draw walls \n",
    "        # added visualization of reaching the goal (green) and hitting a wall (red)\n",
    "        if reward == 1:\n",
    "            rgb = (0, 255, 0)\n",
    "        elif reward == -0.5:\n",
    "            rgb = (255, 0, 0)\n",
    "        else:\n",
    "            rgb =(0, 0, 0)\n",
    "\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self._walls_location[i,j] == 1:\n",
    "                    pygame.draw.rect(\n",
    "                        canvas,\n",
    "                        rgb,\n",
    "                        [pix_square_size * i, pix_square_size * j, pix_square_size, pix_square_size]\n",
    "                        )\n",
    "\n",
    "\n",
    "        # draw path if it is not empty\n",
    "        if path:\n",
    "            # get max delta\n",
    "            dq_max = max(path[2::3])\n",
    "            dq_min = min(path[2::3])\n",
    "            for i in range(len(path) // 3):\n",
    "                # get three tuples \n",
    "                x, y, dq = path[i*3:i*3+3]\n",
    "                # only plot legit dqs\n",
    "                if dq > 0:\n",
    "                    pygame.draw.circle(\n",
    "                    canvas,\n",
    "                    (0, 200, 0),\n",
    "                    ((x + 0.5) * pix_square_size, (y + 0.5) * pix_square_size),\n",
    "                    dq / dq_max * pix_square_size / 3,\n",
    "                    )\n",
    "                elif dq < 0:\n",
    "                    pygame.draw.circle(\n",
    "                    canvas,\n",
    "                    (200, 0, 0),\n",
    "                    ((x + 0.5) * pix_square_size, (y + 0.5) * pix_square_size),\n",
    "                    dq / dq_min * pix_square_size / 3,\n",
    "                    )\n",
    "\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode in [\"human\", \"traces\"]:\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "        \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1a: Eligiblity traces intialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = GridWorldEnv(render_mode=\"traces\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample() # User-defined policy function\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2b: Run eligiblity traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = GridWorldEnv(render_mode=\"traces\")\n",
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    a = env.action_space.sample()\n",
    "    obs, r, terminated, truncated, info = env.step(a)\n",
    "    \n",
    "    if terminated:\n",
    "        env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3.2: Model-based Reinforcement Learning (10 P.)**\n",
    "\n",
    "In the last sheet we used Q-Learning and SARSA to find the optimal way to navigate the frozen lake environment. However, the 8x8 version of the environment took much longer than the 4x4 one. Reasons for this were:\n",
    "\n",
    "- Constant epsilon value\n",
    "- Reward only when transitioning to the goal state\n",
    "- Update of values only based on the previous values\n",
    "- \"Back and forth\" during exploration\n",
    "\n",
    "For this task we want to improve the results a lot by using the model-based approaches from the lecture.\n",
    "\n",
    "**Hand in the final code of the whole exercise and also the plotted curves for all subtasks.**\n",
    "\n",
    "1. **(3 P.)** Start by implementing the plain DynaQ-Algorithm. The function should return the final Q-values as well as a list with the cumulative reward for each iteration. Test your implementation with these parameter values:\n",
    "    - $epsilon = 0.5$ (you can also try out other values for epsilon)\n",
    "    - $gamma = 0.9$, $alpha= 0.1$, $N = 10$ (Planning index)\n",
    "    - Initial Q-values $init\\_value = 0.0$ and \n",
    "    - Iterations = 200000\n",
    "    When plotting the results with the provided function you most probably will observe that all the Q-values are still zero. Change the constant value for epsilon to the dynamic $\\epsilon = e^{-i/\\text{max}(i)}$ where i is the current number of iterations. You might need to run this a few times until you see a result but it should find the goal. Since reaching the goal is not guaranteed and the number of iterations is enormous you are going to improve the algorithm to enhance the results.\n",
    "2. **(3 P.)** The rewards here are sparse and the environment is deterministic, therefore add Prioritized Sweeping to the implementation  (use the theta parameter in the function call). You do not need to implement a complex priority queue. Two lists are sufficient. Also, the defaultdict might help you with step 7 d). Test this with the parameters $gamma = 0.9$, $alpha = 0.5$, $N = 10$ (maximum planning iterations) and $theta = 0.01$. What do you observe?\n",
    "3. **(2 P.)** Try now to add a negative reward every time the agent falls into an ice hole. Implement a [custom wrapper](https://gymnasium.farama.org/tutorials/gymnasium_basics/implementing_custom_wrappers/) from the gymnasium framework that adds a settable negative reward r_hole when the agent falls into an ice hole. Test this with r_hole values of [0, -0.01, -0.05, -0.1, -0.5, -1, -2] and the same parameters as before, recording the cumulative rewards for the episodes. Plot the curves for the rewards.\n",
    "4. **(2 P.)** At last, incorporate the idea of balanced wandering into the $\\epsilon$-greedy policy. Right now, exploration means choosing a random action. Your version should instead choose the action that was chosen the least so far when the state is unknown (in the sense of the the $E^3$-Algorithm). Also test this with different values and parameters of your choice. Hand in the plot of the parameter combination with the best result using the provided plotting function.\n",
    "\n",
    "*Optional:* Try out different frozen lake maps by using the random map generator:\n",
    "\n",
    "```python\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "gym.make('FrozenLake-v1', desc=generate_random_map(size=8, seed=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2a: Initialising Model based RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to use the already imported modules, but they might help you\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Hint for using defaultdict\n",
    "#\n",
    "# You can play around with the following few lines \n",
    "#\n",
    "# from collections import defaultdict\n",
    "# test = defaultdict(dict)\n",
    "# test[\"a\"][\"a\"] = 5\n",
    "# test[\"a\"][\"a\"] = 6\n",
    "# test[\"a\"][\"b\"] = 6\n",
    "# test[\"b\"][\"b\"] = 6\n",
    "# print(test)\n",
    "\n",
    "class MBLearner:\n",
    "\n",
    "    # suggested init\n",
    "    def __init__(self, env, size = (8,8)):\n",
    "        self.env = env\n",
    "        self.size = size\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "    def e_greedy(self, state, epsilon, wander):\n",
    "        # to be implemented\n",
    "        action = None\n",
    "        return action\n",
    "                       \n",
    "    # Keep stubs the same!\n",
    "    def learning(self, iterations, N, balanced_wandering=0, theta = 0.1, alpha = 0.5, gamma = 0.9):\n",
    "        # to be implemented\n",
    "        \n",
    "        # presets for plotting\n",
    "        # Keep type and dimensions the same!\n",
    "        self.reward = [0]\n",
    "        self.q_values = np.zeros((self.size[0] * self.size[1], self.env.action_space.n))\n",
    "\n",
    "        return self.q_values.copy(), self.reward\n",
    "\n",
    "\n",
    "    ###############################\n",
    "    #       Only for plotting\n",
    "    ###############################\n",
    "    def plot_q_values_map(self):\n",
    "        \"\"\"Plot the last frame of the simulation and the policy learned.\"\"\"\n",
    "        qtable_val_max, qtable_directions = self._qtable_directions_map(self.size)\n",
    "\n",
    "        # Plot the last frame\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "        ax[0].imshow(self.env.render())\n",
    "        ax[0].axis(\"off\")\n",
    "        ax[0].set_title(\"Last frame\")\n",
    "\n",
    "        # Plot the policy\n",
    "        sns.heatmap(\n",
    "            qtable_val_max,\n",
    "            annot=qtable_directions,\n",
    "            fmt=\"\",\n",
    "            ax=ax[1],\n",
    "            cmap=sns.color_palette(\"Blues\", as_cmap=True),\n",
    "            linewidths=0.7,\n",
    "            linecolor=\"black\",\n",
    "            xticklabels=[],\n",
    "            yticklabels=[],\n",
    "            annot_kws={\"fontsize\": \"xx-large\"},\n",
    "        ).set(title=\"Learned Q-values\\nArrows represent best action\")\n",
    "        for _, spine in ax[1].spines.items():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_linewidth(0.7)\n",
    "            spine.set_color(\"black\")\n",
    "        img_title = f\"frozenlake_q_values_{self.size[0]}x{self.size[1]}.png\"\n",
    "        fig.savefig(img_title, bbox_inches=\"tight\")\n",
    "\n",
    "        # Plot the cumulative reward\n",
    "        ax[2].plot(self.reward)\n",
    "        ax[2].set_xlabel(\"Iterations\")\n",
    "        ax[2].set_title(\"Cumulative reward\")\n",
    "        ax[2].axhline(0, 0, len(self.reward), linestyle=\"dashed\", color=\"black\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def _qtable_directions_map(self, map_size):\n",
    "        \"\"\"Get the best learned action & map it to arrows.\"\"\"\n",
    "        qtable_val_max = self.q_values.max(axis=1).reshape(map_size)\n",
    "        qtable_best_action = np.argmax(self.q_values, axis=1).reshape(map_size)\n",
    "        directions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "\n",
    "        qtable_directions = np.empty(qtable_best_action.flatten().shape, dtype=str)\n",
    "        eps = np.finfo(float).eps  # Minimum float number on the machine\n",
    "        for idx, val in enumerate(qtable_best_action.flatten()):\n",
    "            if qtable_val_max.flatten()[idx] > eps or qtable_val_max.flatten()[idx] < -eps:\n",
    "                # Assign an arrow only if a minimal Q-value has been learned as best action\n",
    "                # otherwise since 0 is a direction, it also gets mapped on the tiles where\n",
    "                # it didn't actually learn anything\n",
    "                qtable_directions[idx] = directions[val]\n",
    "        qtable_directions = qtable_directions.reshape(map_size)\n",
    "        return qtable_val_max, qtable_directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2b: Run model based RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# initialize base environment\n",
    "base_env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\", desc=None, map_name=\"8x8\", is_slippery=False)\n",
    "base_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Please upload your submission via StudIP by 20:00 on May 22, 2024. If you encounter any issues with the upload process, please contact me in advance at laux@uni-bremen.de.\n",
    "\n",
    "Your submission must include: \n",
    "- All the files you created or modified for your submission\n",
    "- A small .txt file with the names and e-mail addresses of the contributing team members\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
