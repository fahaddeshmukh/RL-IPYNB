{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>Reinforcement Learning Summer 2024</h2>\n",
    "    <h2>Prof. Dr. Frank Kirchner</h2>\n",
    "    <h4>Exercise Sheet â€“ II</h4>\n",
    "    <h5>Due: 08.05.24</h5>\n",
    "    <hr>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.1: Monte Carlo Methods (10 Points)\n",
    "\n",
    "In this task, you are going to implement Monte Carlo learning for a blackjack environment. The blackjack environment follows these rules:\n",
    "\n",
    "- The agent and the dealer both get an open card at the start.\n",
    "- The observation space consists of the value of the dealer and the agent.\n",
    "- The agent starts by drawing cards or sticking with the current cards (the actions).\n",
    "- Then the dealer draws cards until he beats or draws the agent or busts (exceeding 21).\n",
    "- There is no special ace, but the card values 1 and 11.\n",
    "- The card value from 1 to 9 and 11 each have a chance of $1/14$ and the card value 10 has the chance of $4/14$. The drawing happens with replacement.\n",
    "- The environment terminates when a game ends, either by the player winning when reaching 21 (reward: $1.5$) or the dealer exceeding 21 (reward: 1),\n",
    "  the dealer winning by having a higher number than the player or the player busting (both reward: $-1$), or a draw when both have the same value (reward: 0).\n",
    "- Resetting the environment sets the starting values of the dealer and the player to values between 1 and 11.\n",
    "\n",
    "For the following tasks hand in the final code after the last task as well as the plot of the last subtask.\n",
    "\n",
    "1. **(3 Points)** Implement the environment in the cell `2.1a` Black jack. The environment must not be implemented with the gymnasium package but has\n",
    "   to follow the conventions of their environments (names, methods, returned values, etc.). Rendering is not necessary as the results can be output via the print function.\n",
    "   Test your implementation thoroughly.\n",
    "2. **(3 Points)** Implement the First Visit Monte Carlo learning from the lecture in the subsequent `2.1b` cell. We will use the state-action-values for this task and\n",
    "   you should use the predefined variable for that. Run the learning for different number of episodes until you are happy with the success.\n",
    "3. **(1 Point)** Implement an automatic_game(number_of_games) function in the cell `2.1c`, that uses the obtained state-action-values to play against the dealer.\n",
    "4. **(2 Points)** Also implement the every visit MC. Add an attribute in the class initialization method to choose the method.\n",
    "5. **(1 Point)** Train one first visit and one every visit MC for 1000 episodes. Every 100 episodes play 100 automatic games with the obtained\n",
    "   action-state-values. Plot the development of the rewards of both versions in two separate graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1a: Black jack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BlackJackEnv:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.observation_space = None\n",
    "        self.action_space = None\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1b: Monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.q_values = np.zeros((21, 21, 2)) # use this for the state action values\n",
    "\n",
    "    def learning(self, episodes):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1c: Run monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def automatic_game(env, q_values, n):\n",
    "    # complete this \n",
    "\n",
    "\n",
    "env = BlackJackEnv()\n",
    "learner1 = MonteCarlo(env)\n",
    "\n",
    "# complete this and plot the episode wise rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.2: Temporal Difference Learning (10 Points)\n",
    "\n",
    "In this exercise, we use the frozen lake environment of the gymnasium framework to implement both temporal difference algorithms from the lecture. During the tasks, think about the following questions regarding the two algorithms. (You do not need to hand in answers to the questions, this is meant to guide your own understanding.)\n",
    "\n",
    "- Which agent reaches the goal (terminal state) faster?\n",
    "- How do the two methods differ with regard to the obtained returns?\n",
    "- Which algorithm learns faster and why?\n",
    "\n",
    "Hand in the final code of the whole exercise as well as a csv-file of the final state-action-values of each subtask.\n",
    "**Make sure to include the environment name, number of episodes, and the update rule in the csv-file names.**\n",
    "\n",
    "1. **(4 Points)** Read through the [documentation](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) and \n",
    "   implement both the SARSA-Algorithms and the Q-Learning by Watkins in the provided TemporalDifference.py. Do not modify the function parameters and the returned values. \n",
    "   The base configuration of the agent for the experiments should be as follows:\n",
    "   - $epsilon\\_decay = 1.0$\n",
    "   - $gamma = 1.0$ (the discount factor)\n",
    "   - Initial Q-values $init\\_value = 0.0$ and $alpha= 0.1$ (learning rate)\n",
    "   Test your implementation with the frozen lake environment for 2000 episodes and give out the final state-action-values.\n",
    "2. **(1 Point)** Run the test with the `is_slippery` parameter set to True. You may need to increase the number of\n",
    "   episodes to 5000 to see the difference. Print out the state-action-values. Think about the results.\n",
    "3. **(1 Point)** Set the size of the frozen lake environment to 8x8 with the `is_slippery` parameter set to False. Increase the number of\n",
    "   episodes until you see a meaningful result in the state-action-values (This might not work). What are your thoughts on this? How could you improve the learning time?\n",
    "4. **(2 Points)** Adjust your implementation so that it also works with the [cliff walking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) environment.\n",
    "   Test it with 200, 500, and 1000 episodes. Why is the progress for this environment faster? How do the best actions change with the increased number of episodes?\n",
    "   Save the results of the 1000 episode test.\n",
    "5. **(2 Points)** Add a visualization function which plots the final state next to the grid where the best directions are shown (see Figure below).\n",
    "   Run the tests again and verify that the visualization works properly for all settings. Include one graphic for both the frozen lake and the cliff walking.\n",
    "   *Hint: You can use the heatmap from the seaborn package with annotations.*\n",
    "\n",
    "*Frozen Lake results with WatkinsQ and 2000 episodes.*\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"frozen.png\" alt=\"Frozen Lake results with WatkinsQ and 2000 episodes\" width=\"900\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2a: Temporal difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class TDLearning:\n",
    "\n",
    "    def __init__(self, env, env_name=\"frozenlake\"):\n",
    "        self.env = env\n",
    "        self.env_name = env_name\n",
    "        self.action_states = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "\n",
    "    def e_greedy(self, state, epsilon):\n",
    "        pass\n",
    "        #return action\n",
    "    \n",
    "    def learning(self, episodes, update_rule='SARSA', alpha=1.0, gamma=1.0, init_value=0.0, epsilon=0.5, epsilon_decay=1.0):\n",
    "        if update_rule not in ['SARSA', 'WatkinsQ']:\n",
    "            sys.exit('Unknown update_rule, use either \\'SARSA\\' or \\'WatkinsQ\\' !')\n",
    "        #return self.action_states, trajectories\n",
    "    \n",
    "    \n",
    "    def plot_q_values_map(self, map_size):\n",
    "       pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run temporal difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# make environment with chosen options\n",
    "\n",
    "#env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\", desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "#learner = TDLearning(env)\n",
    "\n",
    "#env = gym.make('CliffWalking-v0', render_mode=\"rgb_array\")\n",
    "#learner = TDLearning(env, \"cliffwalking\")\n",
    "\n",
    "learner.learning(2000, \"SARSA\",alpha=0.1)\n",
    "learner.plot_q_values_map((4,4))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Please upload your submission via StudIP by 20:00 on May 08, 2024. If you encounter any issues with the upload process, please contact me in advance at laux@uni-bremen.de.\n",
    "\n",
    "Your submission must include: \n",
    "- All python files you created or modified for your submission\n",
    "- A small .txt file with the names and e-mail addresses of the contributing team members\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
