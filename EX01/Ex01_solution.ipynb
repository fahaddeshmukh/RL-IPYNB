{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h2>Reinforcement Learning Summer 2024</h2>\n",
    "    <h2>Prof. Dr. Frank Kirchner</h2>\n",
    "    <h4>Exercise Sheet – I</h4>\n",
    "    <h5>Due: 24.04.24</h5>\n",
    "    <hr>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.1 (Installation & Framework)\n",
    "\n",
    "For this course, we make use of the **gymnasium framework**. Gymnasium is a standard API for reinforcement learning and also provides a broad collection of environments we will discuss during this course. The documentation of the framework can be found at [gymnasium.farama.org](http://gymnasium.farama.org).\n",
    "\n",
    "Start by installing the main framework and all the environments via the anaconda terminal with these commands:\n",
    "- pip install gynmasium\n",
    "- pip install gynmasium[all]\n",
    "\n",
    "\n",
    "### a) Try to run the code from the main page of the gymnasium documentation. \n",
    "\n",
    "You can fix possible errors related to Microsoft Visual C++ 14.0 by downloading the Microsoft C++ Build Tools and installing the missing package.\n",
    "\n",
    "**Remark**: Visualization is not possible on server-based IDEs like Google Colab.\n",
    "\n",
    "### b) Make yourself familiar with the gymnasium API, especially with the `Env` and the `Spaces` parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.2 (Markov Decision Processes) (10 P.)\n",
    "Using the framework you are now supposed to implement a simple environment\n",
    "yourself with the help of this \n",
    "[tutorial](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/). (You do not need \n",
    "to run the code in the beginning)\n",
    "\n",
    "We have a point robot with simplified motor actions: move forward, turn 90 degrees right, and\n",
    "turn 90 degrees left. All actions can be tried in all states. A simple version of the robot world and \n",
    "its states are shown in Figure 1. The robot can assume four states for a given position shown by the arrows\n",
    "indicating the orientations of the robot (see state definition in Table 1). In the table N, E, S, and W\n",
    "stand for north, east, south, and west, respectively. If the robot is in state 0 and executes the action\n",
    "move forward, then the state of the environment does not change since the robot moves against the\n",
    "world boundary\n",
    "\n",
    "**Figure 1: The Robot world**\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"RobotWorld.jpg\" alt=\"Robot world\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "**Table 1: The state definition of the perceived states.**\n",
    "\n",
    "| **State** | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |\n",
    "|-----------|---|---|---|---|---|---|---|---|---|---|----|----|----|----|----|----|\n",
    "| **Position** | I | I | I | I | II | II | II | II | III | III | III | III | IV | IV | IV | IV |\n",
    "| **Orientation** | N | E | S | W | N | E | S | W | N | E | S | W | N | E | S | W |\n",
    "\n",
    "The task of the robot is to reach a given terminal state by executing a minimum number of actions. In this exercise, we take state 15 as a terminal state. So the robot has to reach the \n",
    "the fourth position oriented in the west direction. The dynamics of the environment are given by \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P_{ss'}^a = \\begin{cases}\n",
    "        1 & \\text{if $s'$ is a valid next state} \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    R_{ss'}^a = \\begin{cases}\n",
    "        -1 & \\text{if $s' = s$ and $s' \\ne $ terminal state}  \\\\\n",
    "        1 & \\text{if $s' \\ne s$ and $s' = $ terminal state}\\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases} \\quad (1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $ P_{ss'}^a $ is the state transition probability and $ R_{ss'}^a $ is the expected immediate reward. One can easily see that the robot is discouraged to take actions against the world boundary.\n",
    "\n",
    "1. **(5 points)** Modify the  cell `1.2a) The robot world` according to the description above. The program should work by running the cell `1.2 b`.\n",
    "2. **(2 points)** Run the test for 1000 steps, resetting every time the robot reaches the terminal state. Save the reward after each action. Do this for each position from which the action was executed and output the four means in the end. Think about the results; do they make sense?    \n",
    "3. **(2 points)** Make the size of the robot world variable so that you can change it while creating the instance of `RobotWorldEnv` class. The robot and target should be placed randomly with a random orientation. Run the test as before, saving the means the same way. \n",
    "4. **(1 point)** Now, add another variable to your environment to include the probability of the robot slipping during a movement. If the agent slips, the action fails and stays in the same state as before. The slip probability variable should specify how likely it is for the robot to slip, thus should take only values between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 a) The Robot world\n",
    "### The following cell contains the definition of the robot environment. It defines the behavior of the robot, including its movement actions, state transitions, rewards, and rendering. You need to modify this cell according the question's description and define the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "\n",
    "class RobotWorldEnv(gym.Env):\n",
    "    # reduced fps for better visualization\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 0.8}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5, slip_prob=0.1, simple_env=False):\n",
    "        # probability that a move is not executed due to slipping\n",
    "        self.slip = slip_prob\n",
    "        # if the simple 1x4 robot world should be created, if true ignores size\n",
    "        self.simple_env = simple_env\n",
    "        # convert size to height and width\n",
    "        if simple_env:\n",
    "            self.width = 4\n",
    "            self.height = 1\n",
    "        elif type(size) == int:\n",
    "            self.width = size\n",
    "            self.height = size\n",
    "        elif type(size) == tuple:\n",
    "            self.height = size[1]\n",
    "            self.width = size[0]\n",
    "        else:\n",
    "            assert False, \"Please provide a valid size\"\n",
    "        \n",
    "        # window size adjusts to grid size\n",
    "        self.window_size = (self.width * 100, self.height * 100) # The size of the PyGame window\n",
    "\n",
    "        # Ad another dimension for the direction\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(np.array([0, 0, 0]) , np.array([self.height - 1, self.width - 1, 3]), dtype=int),\n",
    "                \"target\": spaces.Box(np.array([0, 0, 0]) , np.array([self.height - 1, self.width - 1, 3]), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 3 actions, corresponding to \"forward\", \"turn right\", \"turn left\"\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps the abstract actions 1 and two \n",
    "        to the corresponding changes in the agents location\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            1: np.array([0,0,1]),\n",
    "            2: np.array([0,0,3])\n",
    "        }\n",
    "        \n",
    "        \"\"\"\n",
    "        The following dictionary maps the directions in which the robot is facing to\n",
    "        the movenment it has to take from facing in that direction.\n",
    "        I.e. 0 corresponds to facting north, therefore it has to move 1 up\n",
    "        \"\"\"\n",
    "        self._direction_to_move = {\n",
    "            0: np.array([0,-1,0]),\n",
    "            1: np.array([1,0,0]),\n",
    "            2: np.array([0,1,0]),\n",
    "            3: np.array([-1,0,0])\n",
    "        }\n",
    "\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    # added size to provide it for the functions outside the environment\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            ),\n",
    "            \"size\": (self.width, self.height)\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # simple env has fixed positions\n",
    "        if self.simple_env:\n",
    "            self._agent_location = np.array([0,0,0])\n",
    "            self._target_location = np.array([3,0,3])\n",
    "        else:    \n",
    "            # Choose the agent's location uniformly at random\n",
    "            self._agent_location = self.np_random.integers(0,\n",
    "                                                        [self.width, self.height, 4], \n",
    "                                                        size=3, \n",
    "                                                        dtype=int)\n",
    "\n",
    "            # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "            self._target_location = self._agent_location\n",
    "            while np.array_equal(self._target_location, self._agent_location):\n",
    "                self._target_location = self.np_random.integers(\n",
    "                    0, [self.width, self.height, 4], size=3, dtype=int\n",
    "                )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "\n",
    "        # agent slips and does not move\n",
    "        if self.np_random.random() < self.slip:\n",
    "            pass\n",
    "        elif action == 0:\n",
    "            # if forward, get the corresponding movement from the dictionary\n",
    "            direction = self._direction_to_move[self._agent_location[2]]\n",
    "            tmp = self._agent_location\n",
    "            self._agent_location = np.clip(\n",
    "            self._agent_location + direction, [0,0,0], [self.width - 1, self.height-1, 10]\n",
    "            )\n",
    "            \n",
    "            # if the position before is the same as the position after, reward = -1\n",
    "            if all(self._agent_location == tmp):\n",
    "                reward = -1\n",
    "        else:\n",
    "            # if turn, get the corresponding turn from the dictionary\n",
    "            direction = self._action_to_direction[action]\n",
    "            self._agent_location += direction\n",
    "            # keep the directions between 0 and 3\n",
    "            self._agent_location[2] = self._agent_location[2] % 4\n",
    "        \n",
    "\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        reward = 1 if terminated else reward\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(self.window_size)\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface(self.window_size)\n",
    "        canvas.fill((255, 255, 255))\n",
    "        # pixel sizes are dependend on the height and width\n",
    "        pix_square_height = (\n",
    "            self.window_size[0] / self.width\n",
    "        )  \n",
    "        pix_square_width = (\n",
    "            self.window_size[1] / self.height\n",
    "        ) # The size of a single grid square in pixels\n",
    "        \n",
    "        # First we draw the target\n",
    "        # get the direction of the arrow from the dictionary and draw it from the center of the square\n",
    "        face_direction = self._direction_to_move[self._target_location[2]] * 0.5 + 0.5 + self._target_location\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                (pix_square_height * self._target_location[0], \n",
    "                 pix_square_width * self._target_location[1]),\n",
    "                (pix_square_height, pix_square_width),\n",
    "            ),\n",
    "        )\n",
    "        # draw direction\n",
    "        pygame.draw.line(\n",
    "            canvas,\n",
    "            (100,0,0),\n",
    "            (pix_square_height * (self._target_location[0] + 0.5), \n",
    "             pix_square_width * (self._target_location[1] + 0.5)),\n",
    "            (pix_square_height * face_direction[0], \n",
    "             pix_square_width * face_direction[1]),\n",
    "            width=5\n",
    "        )\n",
    "        \n",
    "        # Now we draw the agent\n",
    "        # get the direction of the arrow from the dictionary and draw it from the center of the square\n",
    "        face_direction = self._direction_to_move[self._agent_location[2]] * 0.4 + self._agent_location + 0.5\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            ((self._agent_location[0] + 0.5) * pix_square_height, (self._agent_location[1] + 0.5) * pix_square_width),\n",
    "            min(pix_square_height, pix_square_width) / 3,\n",
    "        )\n",
    "        pygame.draw.line(\n",
    "            canvas,\n",
    "            (0,0,0),\n",
    "            (pix_square_height * (self._agent_location[0] + 0.5), \n",
    "             pix_square_width * (self._agent_location[1] + 0.5)),\n",
    "            (pix_square_height * face_direction[0], \n",
    "             pix_square_width * face_direction[1]),\n",
    "            width=5\n",
    "        )\n",
    "        \n",
    "        # Finally, add some gridlines\n",
    "        for y in range(self.height + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_height * y),\n",
    "                (self.window_size[0], pix_square_height * y),\n",
    "                width=3,\n",
    "            )\n",
    "        for x in range(self.width + 1):    \n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_width * x, 0),\n",
    "                (pix_square_width * x, self.window_size[1]),\n",
    "                width=3,\n",
    "            )\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 b) Testing the environment\n",
    "\n",
    "### This cell contains test cases to verify the correctness of the robot environment implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dict for saving flags and action assignments\n",
    "attr_dict = {\"slip_flag\": False, \"forward\": None, \"turn1\": None, \"turn2\": None}\n",
    "\n",
    "class TestEnv:\n",
    "    \"\"\"\n",
    "    Test class for testing the simple robot world.\n",
    "    Modify the environment creation to construct the right environment. This can vary from solution to solution.\n",
    "\n",
    "    Single runs of the test functions are not supported.\n",
    "    \"\"\"\n",
    "    def setup_method(self):\n",
    "        # Directly instantiate the environment\n",
    "        self.env = RobotWorldEnv(render_mode=None, simple_env=True)\n",
    "        self.obs, self.info = self.env.reset()\n",
    "\n",
    "    # function to make a move without slipping\n",
    "    def no_slips_step(self, action, agent):\n",
    "        i = 0\n",
    "        step_result = self.env.step(action)\n",
    "        agent = agent.copy()\n",
    "        while step_result[1] == 0 and all(step_result[0][\"agent\"] == agent) and i <= 100:\n",
    "            attr_dict[\"slip_flag\"] = True\n",
    "            step_result = self.env.step(action)\n",
    "            i += 1\n",
    "            if i == 100:\n",
    "                assert False, \"Always slipping, unable to run test\"\n",
    "        return step_result\n",
    "\n",
    "    # tests whether the functions are in the environment\n",
    "    def test_step(self):\n",
    "        assert hasattr(self.env, \"step\") and callable(self.env.step)\n",
    "\n",
    "    def test_render(self):\n",
    "        assert hasattr(self.env, \"render\") and callable(self.env.render)\n",
    "\n",
    "    # test which makes sure there are three actions and that the actions are the right ones\n",
    "    # also saves the values for the different actions\n",
    "    def test_actions(self):\n",
    "        assert self.env.action_space.n == 3\n",
    "        pos_agent = self.obs[\"agent\"].copy()\n",
    "        for action in range(0, 3):\n",
    "            obs, reward, _, _, _ = self.no_slips_step(action, self.obs[\"agent\"])\n",
    "\n",
    "            if reward == -1 or any(pos_agent[:-1] != obs[\"agent\"][:-1]):\n",
    "                attr_dict[\"forward\"] = action\n",
    "\n",
    "            elif reward == 0 and pos_agent[-1] != obs[\"agent\"][-1] and attr_dict[\"turn1\"] is None:\n",
    "                attr_dict[\"turn1\"] = action\n",
    "\n",
    "            elif reward == 0 and pos_agent[-1] != obs[\"agent\"][-1]:\n",
    "                attr_dict[\"turn2\"] = action\n",
    "\n",
    "            else:\n",
    "                assert pos_agent[-1] != obs[\"agent\"][-1] or False\n",
    "\n",
    "            self.env.reset()\n",
    "\n",
    "    # tests whether the\n",
    "    def test_forward(self):\n",
    "        pos_agent = self.obs[\"agent\"].copy()\n",
    "        flag = False\n",
    "        for i in range(3):\n",
    "            obs, reward, _, _, _ = self.no_slips_step(attr_dict[\"forward\"], pos_agent)\n",
    "\n",
    "            if reward == 0:\n",
    "                assert any(pos_agent != obs[\"agent\"])\n",
    "                flag = True\n",
    "                break\n",
    "            else:\n",
    "                pos_agent = self.env.step(attr_dict[\"turn1\"])[0][\"agent\"].copy()\n",
    "\n",
    "        assert flag\n",
    "\n",
    "    def turn_tester(self, action, observations):\n",
    "        flag = True\n",
    "        start_loc = observations[\"agent\"].copy()\n",
    "        pos_agent = start_loc\n",
    "\n",
    "        for _ in range(4):\n",
    "            obs, _, _, _, _ = self.no_slips_step(action, pos_agent)\n",
    "\n",
    "            if all(obs[\"agent\"] == pos_agent):\n",
    "                flag = False\n",
    "                break\n",
    "            pos_agent = obs[\"agent\"].copy()\n",
    "\n",
    "        if not flag and all(pos_agent == start_loc):\n",
    "            flag = False\n",
    "\n",
    "        return flag\n",
    "\n",
    "    def test_turn1(self):\n",
    "        assert self.turn_tester(attr_dict[\"turn1\"], self.obs)\n",
    "\n",
    "    def test_turn2(self):\n",
    "        assert self.turn_tester(attr_dict[\"turn2\"], self.obs)\n",
    "\n",
    "    # tests termination, assumes correct starting locations\n",
    "    def test_termination(self):\n",
    "        flag = False\n",
    "        agent = self.obs[\"agent\"]\n",
    "        for action in list(attr_dict.values())[:-2]:\n",
    "            self.no_slips_step(action, agent)\n",
    "            self.no_slips_step(attr_dict[\"forward\"], agent)\n",
    "            self.no_slips_step(attr_dict[\"forward\"], agent)\n",
    "            self.no_slips_step(attr_dict[\"forward\"], agent)\n",
    "            self.no_slips_step(action, agent)\n",
    "            _, reward, terminated, _, _ = self.no_slips_step(action, agent)\n",
    "\n",
    "            if reward == 1 and terminated:\n",
    "                flag = True\n",
    "                break\n",
    "\n",
    "        assert flag\n",
    "\n",
    "    def test_slip(self):\n",
    "        i = 0\n",
    "        obs = self.obs\n",
    "        while not attr_dict[\"slip_flag\"] and i <= 1000:\n",
    "            tmp = obs[\"agent\"].copy()\n",
    "            obs, reward, _, _, _ = self.env.step(0)\n",
    "\n",
    "            if all(obs[\"agent\"] == tmp) and reward == 0:\n",
    "                attr_dict[\"slip_flag\"] = True\n",
    "\n",
    "            obs, _ = self.env.reset()\n",
    "            i += 1\n",
    "\n",
    "        assert attr_dict[\"slip_flag\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.276]\n",
      " [-0.166]\n",
      " [-0.174]\n",
      " [-0.154]]\n"
     ]
    }
   ],
   "source": [
    "# make environment with chosen options\n",
    "# simple_env overrides the size parameter if True\n",
    "env = RobotWorldEnv(render_mode=None, size=(7,4), slip_prob=0, simple_env=True)\n",
    "observation, info = env.reset()\n",
    "\n",
    "# create array for saving the rewards\n",
    "rewards = np.zeros((info[\"size\"][0], info[\"size\"][1], 2))\n",
    "for i in range(10000):\n",
    "    # save previous location to assign rewars correctly\n",
    "    agent_location = observation[\"agent\"]\n",
    "\n",
    "    # sample one of three actions: 0 - forward, 1 - turn right, 2 - turn left\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # save reward and increment count for the mean\n",
    "    rewards[agent_location[0],agent_location[1], :] += [1, reward]\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "# compute mean\n",
    "rewards = np.nan_to_num(rewards[:,:,1] / rewards[:,:,0]).round(3)\n",
    "# output rewards\n",
    "print(rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.3 (Dynamic Programming) (10 P.)\n",
    "\n",
    "In this problem, you will implement the policy algorithm introduced in the lecture and apply it to the toy example of a vacuum cleaner robot (see: Lecture 2). Use the provided code skeleton in the file ”dynamic programming.py” to implement the algorithm. Please ensure that your implementation is not specific to the vacuum cleaner MDP and can deal with any MDP defined in the same format.\n",
    "\n",
    "**Figure 2**: The vacuum cleaner environment.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"SimpleCleaningRobot.png\" alt=\"Simple cleaning robot\" width=\"600\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "1. **(1 P.)** In the next cells, fill in the missing parts of the vacuum MDP definition in the main function.\n",
    "2. **(4 P.)** Implement the policy iteration algorithm according to the given interface.\n",
    "3. **(2 P.)** Test your implementation and visualise the final policy and value function.\n",
    "4. **(3 P.)** Adapt your implementation to use Q-Values (state-action values) instead of state values to evaluate a given policy. Modify the class constructor to make this choice configurable by the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3  Dynamic programming | PolicyIteration class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PolicyIteration:\n",
    "\n",
    "    def __init__(self, mdp, gamma = 0.3, action_value=False):\n",
    "        \"\"\"\n",
    "        Initialise all necessary variables.\n",
    "        \"\"\"\n",
    "        self.mdp = mdp\n",
    "        self.gamma = gamma\n",
    "        self.action_value = action_value\n",
    "        if action_value:\n",
    "            self.values = np.zeros((len(mdp[\"states\"]), len(mdp[\"actions\"])))\n",
    "        else:\n",
    "            self.values = np.zeros_like(mdp[\"states\"])\n",
    "        self.policy = np.random.choice(mdp[\"actions\"], self.values.shape[0])\n",
    "        self.run()\n",
    "\n",
    "    def policy_evaluation(self, policy, epsilon=1e-5, max_iterations=100):\n",
    "        \"\"\"Performs policy evaluation until max value change is under given threshold (epsilon). Returns the\n",
    "        state-value function for the given policy.\"\"\"\n",
    "    \n",
    "        delta = epsilon\n",
    "        i = 0\n",
    "        while(delta >= epsilon and i < max_iterations):\n",
    "            delta = 0\n",
    "            i+=1\n",
    "            for s in self.mdp[\"states\"]:\n",
    "                a = policy[s]\n",
    "                \n",
    "                if self.action_value:\n",
    "                    q = self.values[s,a]\n",
    "                    self.values[s,a] = self._action_value_function(a,s)\n",
    "                    delta = max(delta, abs(q - self.values[s,a]))\n",
    "                else:\n",
    "                    v = self.values[s]\n",
    "                    self.values[s] = self._state_value_function(a, s)\n",
    "                    delta = max(delta, abs(v - self.values[s]))\n",
    "\n",
    "    def policy_improvement(self, value_function):\n",
    "        \"\"\"Performs policy improvement based on the given value function. Returns the new and improved policy.\"\"\"\n",
    "        stable = True \n",
    "\n",
    "        for s in self.mdp[\"states\"]:\n",
    "            b = self.policy[s]\n",
    "\n",
    "            self.policy[s] = 0 if value_function(0,s) > value_function(1,s) else 1\n",
    "\n",
    "            if b != self.policy[s] and not(s in self.mdp[\"terminal_states\"]):\n",
    "                stable = False\n",
    "        \n",
    "        return stable\n",
    "\n",
    "    def _state_value_function(self, a,s):\n",
    "        return sum([\n",
    "                    self.mdp[\"transition_probabilities\"](a,s,s_prime) * (\n",
    "                    self.mdp[\"reward_function\"](a,s,s_prime) + self.gamma * self.values[s_prime])\n",
    "                    for s_prime in self.mdp[\"states\"]\n",
    "                    ])\n",
    "    \n",
    "    def _action_value_function(self, a, s):\n",
    "        return sum([\n",
    "                    self.mdp[\"transition_probabilities\"](a,s,s_prime) * (\n",
    "                    # the q value is dependent on the state value of s_prime which is the action state value of s_prime and the action unter the current policy\n",
    "                    self.mdp[\"reward_function\"](a,s,s_prime) + self.gamma * self.values[s_prime,self.policy[s_prime]]) \n",
    "                    for s_prime in self.mdp[\"states\"]\n",
    "                    ])\n",
    "        \n",
    "    def run(self, max_iterations=100):\n",
    "        \"\"\"Runs the policy iteration algorithm until convergence or until the max iteration threshold is reached.\"\"\"\n",
    "        stable = False\n",
    "        i = 0\n",
    "        \n",
    "        if self.action_value:\n",
    "            value_function = self._action_value_function\n",
    "        else:\n",
    "            value_function = self._state_value_function\n",
    "\n",
    "        while(not(stable) and i <= max_iterations):\n",
    "            self.policy_evaluation(self.policy)\n",
    "            stable = self.policy_improvement(value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      " [1 0 0 1 1 1]\n",
      "Action State Values:\n",
      "[[0.  0. ]\n",
      " [1.  0. ]\n",
      " [0.1 0. ]\n",
      " [0.  0.5]\n",
      " [0.  5. ]\n",
      " [0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # we define the vacuum robot example from the lecture as an MDP\n",
    "    vacuum_mdp = dict()\n",
    "    vacuum_mdp[\"states\"] = np.arange(6)  # we have six states\n",
    "    vacuum_mdp[\"terminal_states\"] = [0, 5]\n",
    "    vacuum_mdp[\"actions\"] = np.arange(2)  # two available actions: -1 - move left, 1 - move right\n",
    "\n",
    "    vacuum_mdp[\"transition_probabilities\"] = lambda a, s, s_prime : 1 if not(s in vacuum_mdp[\"terminal_states\"]) and a*2-1 + s == s_prime else 0\n",
    "    vacuum_mdp[\"reward_function\"] = lambda a, s, s_prime : 5 if a == 1 and s == 4 else (1 if a == 0 and s == 1 else 0)\n",
    "\n",
    "    vacuum_pi = PolicyIteration(vacuum_mdp, 0.1, True)\n",
    "    print(\"Policy:\\n\", vacuum_pi.policy)\n",
    "    if vacuum_pi.action_value:\n",
    "        print(\"Action State Values:\")\n",
    "    else:\n",
    "        print(\"State Values: \")\n",
    "\n",
    "    print(vacuum_pi.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Please upload your submission via StudIP by 20:00 on April 24, 2024. If you encounter any issues with the upload process, please contact me in advance at laux@uni-bremen.de.\n",
    "\n",
    "Your submission must include: \n",
    "- All python files you created or modified for your submission\n",
    "- A small .txt file with the names and e-mail addresses of the contributing team members\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
